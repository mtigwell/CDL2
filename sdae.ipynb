{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd06b71409ac9d4f1c7db598eb563f55ea71f3829c0066c88ff38d64b6990caf5c0",
   "display_name": "Python 3.7.9 64-bit ('cdl': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading item data...\n",
      "Loading rating matrix...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "# from cdl import CDL\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "    # EMBEDDING SIZE\n",
    "    with open(r\"./data/citeulike/citeulike-a/vocabulary.dat\") as vocabulary_file:\n",
    "        embedding_size = len(vocabulary_file.readlines())\n",
    "\n",
    "    # Create Item Matrix \n",
    "    with open(r\"./data/citeulike/citeulike-a/mult.dat\") as item_info_file:\n",
    "        # initialize item matrix (16980 , 8000)\n",
    "        item_size = len(item_info_file.readlines())\n",
    "        item_bow = np.zeros((item_size , embedding_size))\n",
    "\n",
    "        sentences = item_info_file.readlines()\n",
    "        for index,sentence in enumerate(sentences):\n",
    "            words = sentence.strip().split(\" \")[1:]\n",
    "            for word in words:\n",
    "                vocabulary_index , number = word.split(\":\")\n",
    "                item_bow[index][int(vocabulary_index)] = number\n",
    "\n",
    "    #find user_size = 5551\n",
    "    with open(r\"./data/citeulike/citeulike-a/users.dat\") as rating_file:\n",
    "        user_size = len(rating_file.readlines())\n",
    "\n",
    "    #initialize rating_matrix (5551 , 16980)\n",
    "    import numpy as np\n",
    "    rating_matrix = np.zeros((user_size , item_size))\n",
    "\n",
    "    #build rating_matrix\n",
    "    with open(r\"./data/citeulike/citeulike-a/users.dat\") as rating_file:\n",
    "        lines = rating_file.readlines()\n",
    "        for index,line in enumerate(lines):\n",
    "            items = line.strip().split(\" \")\n",
    "            for item in items:  \n",
    "                rating_matrix[index][int(item)] = 1\n",
    "\n",
    "    with open(r'./data/citeulike/citeulike-a/item_bow.pickle', 'wb') as handle:\n",
    "        pickle.dump(item_bow, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(r'./data/citeulike/citeulike-a/rating_matrix.pickle', 'wb') as handle:\n",
    "        pickle.dump(rating_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "try:\n",
    "    print('Loading item data...')\n",
    "    with open(r'./data/citeulike/citeulike-a/item_bow.pickle', 'rb') as handle:\n",
    "        item_matrix = pickle.load(handle) \n",
    "    print('Loading rating matrix...')\n",
    "    with open(r'./data/citeulike/citeulike-a/rating_matrix.pickle', 'rb') as handle2:\n",
    "        rating_matrix = pickle.load(handle2)\n",
    "except:\n",
    "    print('preprocessing data...')\n",
    "    preprocess_data()\n",
    "    with open(r'./data/citeulike/citeulike-a/item_bow.pickle', 'rb') as handle:\n",
    "        item_matrix = pickle.load(handle) \n",
    "    with open(r'./data/citeulike/citeulike-a/rating_matrix.pickle', 'rb') as handle2:\n",
    "        rating_matrix = pickle.load(handle2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, input_dim, latent_dim, encoder_activation, decoder_activation):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder_activation = encoder_activation\n",
    "        self.decoder_activation = decoder_activation\n",
    "\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Dense(latent_dim, activation=self.encoder_activation),\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(input_dim, activation=self.decoder_activation),\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDAE(Model):\n",
    "    def __init__(self, ae_layers):\n",
    "        super(SDAE, self).__init__()\n",
    "        self.ae_layers = ae_layers\n",
    "        self.models = []\n",
    "\n",
    "    def make(self):\n",
    "        for i, layer in enumerate(self.ae_layers[:-1]):\n",
    "            print('building layer input {} output {}'.format(layer, self.ae_layers[i+1]))\n",
    "            m = Autoencoder(layer, self.ae_layers[i+1], 'relu', 'sigmoid')\n",
    "            # m.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "            self.models.append(Autoencoder(layer, self.ae_layers[i+1], 'relu', 'sigmoid'))\n",
    "\n",
    "    def call(self, train, test, epochs):\n",
    "        train_set = train\n",
    "        test_set = test\n",
    "\n",
    "        for m in self.models:\n",
    "            m.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "            m.fit(train_set, train_set, epochs=epochs, shuffle=True, validation_data=(test_set, test_set))\n",
    "            train_set = m.encode(train_set)\n",
    "            test_set = m.encode(test_set)\n",
    "\n",
    "    def get_layers(self):\n",
    "        model_layers = []\n",
    "        for m in self.models:\n",
    "            w = m.get_weights()\n",
    "            layer_dict = {\n",
    "                'w1': w[0],\n",
    "                'b1': w[1],\n",
    "                'w2': w[2],\n",
    "                'b2': w[3]\n",
    "            }\n",
    "            model_layers.append(layer_dict)\n",
    "        return model_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "building layer input 8000 output 64\n",
      "building layer input 64 output 16\n",
      "Epoch 1/2\n",
      "425/425 [==============================] - 3s 6ms/step - loss: 0.2257 - val_loss: 0.1615\n",
      "Epoch 2/2\n",
      "425/425 [==============================] - 2s 5ms/step - loss: 0.1452 - val_loss: 0.1040\n",
      "Epoch 1/2\n",
      "425/425 [==============================] - 1s 1ms/step - loss: 0.2253 - val_loss: 0.1608\n",
      "Epoch 2/2\n",
      "425/425 [==============================] - 0s 1ms/step - loss: 0.1445 - val_loss: 0.1033\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'CDL' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-326aaca61319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mresult_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'results/test3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCDL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrating_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_u\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_v\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_m\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mcdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# cdl.pretrain()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CDL' is not defined"
     ]
    }
   ],
   "source": [
    "from cdl import CDL\n",
    "\n",
    "SPLIT = 0.8 #80/20\n",
    "split = int(item_matrix.shape[0] * SPLIT)\n",
    "\n",
    "x_train2 = item_matrix[:split]\n",
    "x_test2 = item_matrix[split:]\n",
    "\n",
    "ae_layers = [8000, 64, 16]\n",
    "sdae = SDAE(ae_layers)\n",
    "sdae.make()\n",
    "sdae.call(x_train2, x_test2, epochs=2)\n",
    "trained_model = sdae.get_layers()\n",
    "\n",
    "result_directory = 'results/test3'\n",
    "cdl = CDL(rating_matrix, item_matrix, lambda_u=1, lambda_v=10, lambda_w=10, lv=0.01, K=K, epochs=10, batch=batch, \n",
    "        dir_save=result_directory, dropout=dropout, recall_m=100, trained_matrix=trained_model, pretrain=1\n",
    "    )\n",
    "cdl.build_model()\n",
    "cdl.training(rating_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### VARIABLES AS DECLARED IN MATLAB CODE FOR CDL ####\n",
    "layers = [8000, 200, 50]\n",
    "lv = 10\n",
    "lu = 1\n",
    "ln = 1e3\n",
    "pretrain = 1\n",
    "dropout = 0.1\n",
    "sdae_epochs = 10 # may be a min epochs\n",
    "minibatch = 128 #or 256\n",
    "tanh = 1 # used for all layers except the first\n",
    "\n",
    "# sdae parameters for pretrain\n",
    "pretrain_learning_rate = 1e-5 # if tanh, else lr = 1e-1\n",
    "use_adadelta = 0\n",
    "learning_rate0 = 5000 # not sure what this is\n",
    "weight_decay = 1e-4\n",
    "min_epochs = 10\n",
    "sparsity_cost = 0.1\n",
    "epsilon = 1e-8\n",
    "momentum = 0.99\n",
    "\n",
    "# main cdl\n",
    "learning_rate = 1e-6 #if tanh, else lr = 1e-1\n",
    "learning_rate0 = 5000\n",
    "use_adadelta = 0\n",
    "\n",
    "# use dropout on all but the bottleneck layer\n",
    "dropout = 0.1 #paper\n",
    "noise = 0.3 #paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}