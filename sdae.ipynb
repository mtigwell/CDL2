{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd06b71409ac9d4f1c7db598eb563f55ea71f3829c0066c88ff38d64b6990caf5c0",
   "display_name": "Python 3.7.9 64-bit ('cdl': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading item data...\n",
      "Loading rating matrix...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "# from cdl import CDL\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "    # EMBEDDING SIZE\n",
    "    with open(r\"./data/citeulike/citeulike-a/vocabulary.dat\") as vocabulary_file:\n",
    "        embedding_size = len(vocabulary_file.readlines())\n",
    "\n",
    "    # Create Item Matrix \n",
    "    with open(r\"./data/citeulike/citeulike-a/mult.dat\") as item_info_file:\n",
    "        # initialize item matrix (16980 , 8000)\n",
    "        item_size = len(item_info_file.readlines())\n",
    "        item_bow = np.zeros((item_size , embedding_size))\n",
    "\n",
    "        sentences = item_info_file.readlines()\n",
    "        for index,sentence in enumerate(sentences):\n",
    "            words = sentence.strip().split(\" \")[1:]\n",
    "            for word in words:\n",
    "                vocabulary_index , number = word.split(\":\")\n",
    "                item_bow[index][int(vocabulary_index)] = number\n",
    "\n",
    "    #find user_size = 5551\n",
    "    with open(r\"./data/citeulike/citeulike-a/users.dat\") as rating_file:\n",
    "        user_size = len(rating_file.readlines())\n",
    "\n",
    "    #initialize rating_matrix (5551 , 16980)\n",
    "    import numpy as np\n",
    "    rating_matrix = np.zeros((user_size , item_size))\n",
    "\n",
    "    #build rating_matrix\n",
    "    with open(r\"./data/citeulike/citeulike-a/users.dat\") as rating_file:\n",
    "        lines = rating_file.readlines()\n",
    "        for index,line in enumerate(lines):\n",
    "            items = line.strip().split(\" \")\n",
    "            for item in items:  \n",
    "                rating_matrix[index][int(item)] = 1\n",
    "\n",
    "    with open(r'./data/citeulike/citeulike-a/item_bow.pickle', 'wb') as handle:\n",
    "        pickle.dump(item_bow, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(r'./data/citeulike/citeulike-a/rating_matrix.pickle', 'wb') as handle:\n",
    "        pickle.dump(rating_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "try:\n",
    "    print('Loading item data...')\n",
    "    with open(r'./data/citeulike/citeulike-a/item_bow.pickle', 'rb') as handle:\n",
    "        item_matrix = pickle.load(handle) \n",
    "    print('Loading rating matrix...')\n",
    "    with open(r'./data/citeulike/citeulike-a/rating_matrix.pickle', 'rb') as handle2:\n",
    "        rating_matrix = pickle.load(handle2)\n",
    "except:\n",
    "    print('preprocessing data...')\n",
    "    preprocess_data()\n",
    "    with open(r'./data/citeulike/citeulike-a/item_bow.pickle', 'rb') as handle:\n",
    "        item_matrix = pickle.load(handle) \n",
    "    with open(r'./data/citeulike/citeulike-a/rating_matrix.pickle', 'rb') as handle2:\n",
    "        rating_matrix = pickle.load(handle2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, input_dim, latent_dim, encoder_activation, decoder_activation):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder_activation = encoder_activation\n",
    "        self.decoder_activation = decoder_activation\n",
    "\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Dense(latent_dim, activation=self.encoder_activation),\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(input_dim, activation=self.decoder_activation),\n",
    "        ])\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDAE(Model):\n",
    "    def __init__(self, ae_layers):\n",
    "        super(SDAE, self).__init__()\n",
    "        self.ae_layers = ae_layers\n",
    "        self.models = []\n",
    "\n",
    "    def make(self):\n",
    "        for i, layer in enumerate(self.ae_layers[:-1]):\n",
    "            print('building layer input {} output {}'.format(layer, self.ae_layers[i+1]))\n",
    "            m = Autoencoder(layer, self.ae_layers[i+1], 'relu', 'sigmoid')\n",
    "            # m.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "            self.models.append(Autoencoder(layer, self.ae_layers[i+1], 'relu', 'sigmoid'))\n",
    "\n",
    "    def call(self, train, test, epochs):\n",
    "        train_set = train\n",
    "        test_set = test\n",
    "\n",
    "        for m in self.models:\n",
    "            m.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "            m.fit(train_set, train_set, epochs=epochs, shuffle=True, validation_data=(test_set, test_set))\n",
    "            train_set = m.encode(train_set)\n",
    "            test_set = m.encode(test_set)\n",
    "\n",
    "    def get_layers(self):\n",
    "        model_layers = []\n",
    "        for m in self.models:\n",
    "            w = m.get_weights()\n",
    "            layer_dict = {\n",
    "                'w1': w[0],\n",
    "                'b1': w[1],\n",
    "                'w2': w[2],\n",
    "                'b2': w[3]\n",
    "            }\n",
    "            model_layers.append(layer_dict)\n",
    "        return model_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "building layer input 8000 output 64\n",
      "building layer input 64 output 16\n",
      "Train on 13584 samples, validate on 3396 samples\n",
      "Epoch 1/2\n",
      "13584/13584 [==============================] - ETA: 0s - loss: 0.2032/Users/mtigwell/opt/anaconda3/envs/cdl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "13584/13584 [==============================] - 3s 200us/sample - loss: 0.2032 - val_loss: 0.1615\n",
      "Epoch 2/2\n",
      "13584/13584 [==============================] - 2s 179us/sample - loss: 0.1306 - val_loss: 0.1040\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "When using data tensors as input to a model, you should specify the `steps_per_epoch` argument.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-8fc4a68a6628>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0msdae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSDAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msdae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msdae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CDL/CDL/SDAE.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, train, test, epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cdl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cdl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cdl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2293\u001b[0m     \u001b[0;31m# Validates `steps` argument based on x's type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m       \u001b[0mtraining_utils_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_steps_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m     \u001b[0;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cdl/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils_v1.py\u001b[0m in \u001b[0;36mcheck_steps_argument\u001b[0;34m(input_data, steps, steps_name)\u001b[0m\n\u001b[1;32m   1202\u001b[0m       raise ValueError('When using {input_type} as input to a model, you should'\n\u001b[1;32m   1203\u001b[0m                        ' specify the `{steps_name}` argument.'.format(\n\u001b[0;32m-> 1204\u001b[0;31m                            input_type=input_type_str, steps_name=steps_name))\n\u001b[0m\u001b[1;32m   1205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: When using data tensors as input to a model, you should specify the `steps_per_epoch` argument."
     ]
    }
   ],
   "source": [
    "from CDL.cdl import CDL\n",
    "from CDL.SDAE import SDAE\n",
    "\n",
    "SPLIT = 0.8 #80/20\n",
    "split = int(item_matrix.shape[0] * SPLIT)\n",
    "\n",
    "x_train2 = item_matrix[:split]\n",
    "x_test2 = item_matrix[split:]\n",
    "\n",
    "ae_layers = [8000, 64, 16]\n",
    "sdae = SDAE(ae_layers)\n",
    "sdae.make()\n",
    "sdae.call(x_train2, x_test2, epochs=2)\n",
    "trained_model = sdae.get_layers()\n",
    "\n",
    "K = 50\n",
    "batch = 256\n",
    "dropout = 0.1\n",
    "\n",
    "result_directory = 'results/test3'\n",
    "cdl = CDL(rating_matrix, item_matrix, lambda_u=1, lambda_v=10, lambda_w=10, lv=0.01, K=K, epochs=10, batch=batch, \n",
    "        dir_save=result_directory, dropout=dropout, recall_m=100, trained_matrix=trained_model, pretrain=1\n",
    "    )\n",
    "cdl.build_model()\n",
    "cdl.training(rating_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### VARIABLES AS DECLARED IN MATLAB CODE FOR CDL ####\n",
    "layers = [8000, 200, 50]\n",
    "lv = 10\n",
    "lu = 1\n",
    "ln = 1e3\n",
    "pretrain = 1\n",
    "dropout = 0.1\n",
    "sdae_epochs = 10 # may be a min epochs\n",
    "minibatch = 128 #or 256\n",
    "tanh = 1 # used for all layers except the first\n",
    "\n",
    "# sdae parameters for pretrain\n",
    "pretrain_learning_rate = 1e-5 # if tanh, else lr = 1e-1\n",
    "use_adadelta = 0\n",
    "learning_rate0 = 5000 # not sure what this is\n",
    "weight_decay = 1e-4\n",
    "min_epochs = 10\n",
    "sparsity_cost = 0.1\n",
    "epsilon = 1e-8\n",
    "momentum = 0.99\n",
    "\n",
    "# main cdl\n",
    "learning_rate = 1e-6 #if tanh, else lr = 1e-1\n",
    "learning_rate0 = 5000\n",
    "use_adadelta = 0\n",
    "\n",
    "# use dropout on all but the bottleneck layer\n",
    "dropout = 0.1 #paper\n",
    "noise = 0.3 #paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "trained_model[0]['b1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}